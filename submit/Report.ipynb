{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine, Data and Learning\n",
    "\n",
    "## Assignment 2 Part 2\n",
    "\n",
    "**Vasu Singhal (2018101074)**\n",
    "<br/>\n",
    "\n",
    "**Tanish Lad (2018114005)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for value iteration algorithm are initialised below.\n",
    "\n",
    "Team Number is 32, hence Y = 2, and hence for task 1, Penalty = -5.\n",
    "\n",
    "The value of gamma, delta, and step cost is changed accordingly for task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_h = 5\n",
    "num_a = 4\n",
    "num_s = 3\n",
    "gamma = 0.99\n",
    "delta = 1e-3\n",
    "step_cost = {\n",
    "    \"SHOOT\": -5,\n",
    "    \"DODGE\": -5,\n",
    "    \"RECHARGE\": -5\n",
    "}\n",
    "non_terminal_reward = 0\n",
    "terminal_reward = 10\n",
    "inf = 1e17\n",
    "\n",
    "actions = [\"SHOOT\", \"DODGE\", \"RECHARGE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the possible states in which the player can exist are defined below and stored as a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [(health, arrows, stamina) for health in range(num_h)\n",
    "          for arrows in range(num_a)\n",
    "          for stamina in range(num_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print = []\n",
    "all_tasks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**all_state** is a dictionary whose key is a \"state\" and value is the probability. It will be used later to store the probability of reaching a particle state. Currently all the probabilities are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states():\n",
    "    all_state = {}\n",
    "    for health, arrows, stamina in states:\n",
    "        all_state[tuple([health, arrows, stamina])] = 0\n",
    "\n",
    "    return all_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition_prob dictionary stores the probability of transition from current state to the next state via a given action. \n",
    "\n",
    "In other words, **transition_prob[s][a][s']** stores **P(s'|s,a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probabilities():\n",
    "    transition_prob = {}\n",
    "\n",
    "    for health, arrows, stamina in states:\n",
    "        transition_prob[tuple([health, arrows, stamina])] = {\n",
    "            \"SHOOT\": get_states(), \"DODGE\": get_states(), \"RECHARGE\": get_states()}\n",
    "\n",
    "    for health, arrows, stamina in states:\n",
    "        state = tuple([health, arrows, stamina])\n",
    "\n",
    "        # RECHARGE\n",
    "        next_state1 = tuple([health, arrows, stamina])\n",
    "        next_state2 = tuple([health, arrows, min(num_s - 1, stamina + 1)])\n",
    "\n",
    "        transition_prob[state][\"RECHARGE\"][next_state1] += 0.2\n",
    "        transition_prob[state][\"RECHARGE\"][next_state2] += 0.8\n",
    "\n",
    "        # DODGE\n",
    "        if(stamina == 0):\n",
    "            continue\n",
    "\n",
    "        next_state1 = tuple([health, arrows, max(stamina - 1, 0)])\n",
    "        next_state2 = tuple([health, arrows, max(stamina - 2, 0)])\n",
    "        next_state3 = tuple(\n",
    "            [health, min(arrows + 1, num_a - 1), max(stamina - 1, 0)])\n",
    "        next_state4 = tuple(\n",
    "            [health, min(arrows + 1, num_a - 1), max(stamina - 2, 0)])\n",
    "\n",
    "        transition_prob[state][\"DODGE\"][next_state1] += 0.16\n",
    "        transition_prob[state][\"DODGE\"][next_state2] += 0.04\n",
    "        transition_prob[state][\"DODGE\"][next_state3] += 0.64\n",
    "        transition_prob[state][\"DODGE\"][next_state4] += 0.16\n",
    "\n",
    "        # SHOOT\n",
    "        if(arrows == 0):\n",
    "            continue\n",
    "\n",
    "        next_state1 = tuple(\n",
    "            [max(health - 1, 0), max(arrows - 1, 0), max(stamina - 1, 0)])\n",
    "        next_state2 = tuple([max(health, 0), max(\n",
    "            arrows - 1, 0), max(stamina - 1, 0)])\n",
    "\n",
    "        transition_prob[state][\"SHOOT\"][next_state1] += 0.5\n",
    "        transition_prob[state][\"SHOOT\"][next_state2] += 0.5\n",
    "\n",
    "    return transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prob = get_transition_probabilities()\n",
    "# transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **get_utilities** function returns the array new_utilities which stores the utilities of the current iteration.\n",
    "\n",
    "In other words, given **U_{t}**, the function get_utilities will calculate **U_{t+1}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utilities(utilities):\n",
    "    new_utilities = np.zeros(shape=(num_h, num_a, num_s))\n",
    "\n",
    "    for health, arrows, stamina in states:\n",
    "\n",
    "        cur_state = tuple([health, arrows, stamina])\n",
    "\n",
    "        if health == 0:\n",
    "            continue\n",
    "\n",
    "        cur_max = -inf\n",
    "\n",
    "        for action in actions:\n",
    "\n",
    "            if action == \"SHOOT\":\n",
    "                if stamina == 0 or arrows == 0:\n",
    "                    continue\n",
    "\n",
    "            elif action == \"DODGE\" and stamina == 0:\n",
    "                continue\n",
    "\n",
    "            total_reward = 0\n",
    "            cur = 0\n",
    "\n",
    "            for h, a, s in states:\n",
    "\n",
    "                new_state = tuple([h, a, s])\n",
    "\n",
    "                if h == 0:\n",
    "                    total_reward += (step_cost[action] + terminal_reward) * \\\n",
    "                        transition_prob[cur_state][action][new_state]\n",
    "\n",
    "                else:\n",
    "                    total_reward += (step_cost[action] + non_terminal_reward) * \\\n",
    "                        transition_prob[cur_state][action][new_state]\n",
    "\n",
    "                cur += gamma * \\\n",
    "                    transition_prob[cur_state][action][new_state] * \\\n",
    "                    utilities[h, a, s]\n",
    "\n",
    "            cur += total_reward\n",
    "\n",
    "            if cur_max < cur:\n",
    "                cur_max = cur\n",
    "\n",
    "        new_utilities[health, arrows, stamina] = cur_max\n",
    "\n",
    "    return new_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **get_action** calculates which action maximizes the utility at that iteration for every state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(new_utilities):\n",
    "    for health, arrows, stamina in states:\n",
    "\n",
    "        cur_state = tuple([health, arrows, stamina])\n",
    "\n",
    "        if health == 0:\n",
    "            to_print.append(\n",
    "                f\"({health},{arrows},{stamina}):{-1}=[{round(new_utilities[health, arrows, stamina], 3)}]\")\n",
    "            continue\n",
    "\n",
    "        cur_max = -inf\n",
    "        cur_action = \"\"\n",
    "\n",
    "        for action in actions:\n",
    "\n",
    "            if action == \"SHOOT\":\n",
    "                if stamina == 0 or arrows == 0:\n",
    "                    continue\n",
    "\n",
    "            elif action == \"DODGE\" and stamina == 0:\n",
    "                continue\n",
    "\n",
    "            cur = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            for h, a, s in states:\n",
    "                new_state = tuple([h, a, s])\n",
    "\n",
    "                if h == 0:\n",
    "                    total_reward += (step_cost[action] + terminal_reward) * \\\n",
    "                        transition_prob[cur_state][action][new_state]\n",
    "                else:\n",
    "                    total_reward += (step_cost[action] + non_terminal_reward) * \\\n",
    "                        transition_prob[cur_state][action][new_state]\n",
    "\n",
    "                cur += gamma * \\\n",
    "                    transition_prob[cur_state][action][new_state] * \\\n",
    "                    new_utilities[h, a, s]\n",
    "            cur += total_reward\n",
    "            if cur_max < cur:\n",
    "                cur_max = cur\n",
    "                cur_action = action\n",
    "\n",
    "        to_print.append(\n",
    "            f\"({health},{arrows},{stamina}):{cur_action}=[{round(new_utilities[health, arrows, stamina], 3)}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **hasConverged** checks if our model has converged or not.\n",
    "\n",
    "In other words, it calculates the max of each of the absolute values of U_{t+1} - U_{t} and checks if it is less than the given max possible epsillon (which in this case is delta = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasConverged(utilities, new_utilities):\n",
    "    return np.max(np.abs(new_utilities - utilities)) < delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "\n",
    "    utilities = np.zeros(shape=(num_h, num_a, num_s))\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        to_print.append(f\"iteration={iterations}\")\n",
    "\n",
    "        new_utilities = get_utilities(utilities)\n",
    "\n",
    "        get_action(new_utilities)\n",
    "\n",
    "        converged = hasConverged(utilities, new_utilities)\n",
    "\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        utilities = new_utilities\n",
    "        iterations += 1\n",
    "\n",
    "        to_print.append(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**value_iteration** is called below for each task and subparts. The values of parameters are changed according to the task and subparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    if i == 1:\n",
    "        step_cost[\"SHOOT\"] = -0.25\n",
    "        step_cost[\"DODGE\"] = -2.5\n",
    "        step_cost[\"RECHARGE\"] = -2.5\n",
    "\n",
    "    elif i == 2:\n",
    "        step_cost[\"SHOOT\"] = -2.5\n",
    "        gamma = 0.1\n",
    "\n",
    "    elif i == 3:\n",
    "        delta = 1e-10\n",
    "\n",
    "    value_iteration()\n",
    "\n",
    "    all_tasks.append(to_print)\n",
    "    to_print = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in all_tasks:\n",
    "#     for j in i:\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences and Observations\n",
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy that model follows is fairly obvious.\n",
    "Since there is positive reward for the state where health of Mighty dragon is 0, Lero has a tendency to try to achieve such states\n",
    "\n",
    "Lero has to recharge when his stamina is 0 because that is his only option.\n",
    "\n",
    "When Lero has no arrows but has stamina, Lero chooses to dodge so as to gain arrows\n",
    "\n",
    "When Lero has both stamina and arrows, he can either shoot, dodge, or even recharge. If the Mighty Dragon's health is low, Lero shoots. Lero dodges when he feels that he will require more arrows in the future to kill the Dragon. Lero recharges when he feels that he will require more stamina in the future to kill the Mighty Dragon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "Here, the step cost of \"SHOOT\" action is the least negative out of all the three actions, and shooting brings the player closer to terminal states (with high positive reward), and hence there is more incentive for the agent to \"SHOOT\" whenever possible. This change can be observed from the differences in policies obtained in Task 1 and Task 2 Part 1. For example, in the state tuple of (3, 3, 1) and (3, 2, 1) [here, (h, a, s) represents the tuple of health, arrows, stamina], the agent changes its policy from \"RECHARGE\" to \"SHOOT\". Similarly for the state tuple of (4, 2, 1), the agent changes its policy from \"DODGE\" to \"SHOOT\". The model tries to be greedy and \"SHOOTS\" whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "In part 2, the gamma is 0.1, which is a lot lesser than what was in previous tasks (0.99). Smaller gamma means smaller horizon, so shorter term focus, so less preference to future. So whenever he has arrows, he shoots without thinking of the future. This change can be observed from the differences in policies obtained in Task 1 and Task 2 Part 2. For example, in the state tuple of (3, 3, 1) and (3, 2, 1) [here, (h, a, s) represents the tuple of health, arrows, stamina], the agent changes its policy from \"RECHARGE\" to \"SHOOT\". Similarly for the state (4, 2, 1), the agent changes its policy from \"DODGE\" to \"SHOOT\". The agent doesn't think whether he will have enough stamina or enough arrows in future to kill the dragon, he just tries to get as much closer to any one of the good terminal states as possible. Also, the number of iterations it takes to converge gets very low due to very less gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "Little difference can be observed in Part 3 compared to the outputs of Part 2, except for the fact that the number of iterations required to converge have increased due to a lot lesser delta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
